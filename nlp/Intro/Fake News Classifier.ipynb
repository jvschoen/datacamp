{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning with NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use BOW models or tf-idf as features  \n",
    "Example: Predict Sci-Fi or Action based on plot summary  \n",
    "Features: num words, named entities, language\n",
    "\n",
    "* Dataset consists of movie plots and genre  \n",
    "* Goal: create bag-of-word vectors for movie plots  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ellipsis' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d214d87d2c01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sci-Fi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[1;32m      4\u001b[0m     df['plot'], y, test_size=0.33, random_state=53)\n\u001b[1;32m      5\u001b[0m \u001b[0mcount_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ellipsis' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "df = ...\n",
    "y = df['Sci-Fi']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['plot'], y, test_size=0.33, random_state=53)\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Create bag of words vector, mapping of words and ids and vectors \n",
    "# representing how many times each word appears in plot\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "count_test = count_vectorizer.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
    "\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "print(tfidf_test.A[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to DFs to get a better idea of what's in a vector\n",
    "count_df = pd.DataFrame(count_train.A, \n",
    "                        columns=count_vectorizer.get_feature_names())\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, \n",
    "                        columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given data how likely is outcome.  \n",
    "Each word from CountVectorizer acts as a feature  \n",
    "**Doesn't** work well with floats, use SVM or linear models.  \n",
    "Always good idea to test NB first to determine if works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf = MutinomialNB()\n",
    "## count_vectorizer\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "pred = nb_classifier.predict(count_test)\n",
    "metrics.accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test, pred, labels=[0,1])\n",
    "[['tp', 'fp'],\n",
    " ['fn', 'tn']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NLP, complex problems\n",
    "* Translation has a long way to go  \n",
    "    * Word vectors between language don't always align with more intricate differences\n",
    "* Sentiment analysis\n",
    "    * Sarcasm, \"I liked it but it could have been better\"  \n",
    "* Predjudice and Biases  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving model\n",
    "Tweaking alpha levels with tfidf vectors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(0, 1, 0.1)\n",
    "def train_and_predict(alpha):\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the model\n",
    "Map important vector weights back to acutal words using inspection techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = nb.classifier.classes_\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print first class label (FAKE) and top 20 feat with weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "# Print second class label (REAL) and bottom 20 feature weights\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacamp",
   "language": "python",
   "name": "datacamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
